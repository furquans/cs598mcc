% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009
\documentclass{acm_proc_article-sp}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}
\newcommand{\tup}[1]{\langle #1\rangle} 
\begin{document}

\title{Improving Live Migration: is it even worth it?}
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Zachary Estrada
\email zestrad2@illinois.edu
\alignauthor
Furquan Shaikh
\email fmshaik2@illinois.edu
}

\maketitle
\begin{abstract}
Virtualization technology is ubiquitous in most of today's datacenters and is an enabling technology for Infrastructure-as-a-Service Cloud Computing.  As the size of VM farms grows, concerns about the scalability of managing this infrastructure is growing.  Live migration techniques offer large flexibility gains at the cost of increased network traffic.  One solution to reduce this traffic involves exploiting the redundancy of data across virtual machines by migrating groups of similar virtual machines.  For this to be a worthwhile endeavor, we must first understand the nature of this redundancy.  The goal of this study is to investigate some possible use cases and determine the amount of memory redundancy that exists across VMs for these cases.  We develop a platform for evaluating the similarity of VM memory images and then utilize it to gain insight into the level of redundancy that can be exploited in these systems.
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
\setlength{\parindent}{0.5cm}
\section{Introduction}
Virtualization has become one of the most important technologies today. It plays a crucial role in many distributed systems and services. It also forms the fundamental block of Infrastructure-as-a-Service cloud computing systems, such as Amazon EC2 \cite{ec2} and Windows Azure\cite{azure}. The reason behind virtualization's prevalence in the data center is that it can help Systems Administrators better utilize resources and reduce costs by allowing multiple operating systems to run concurrently on the same physical infrastructure.  Multiple operating systems running in their own virtual machines share hardware resources, while ensuring high availability for the user applications since barring hypervisor failure, there is reduced fate sharing (vs. running multiple applications in the same OS) due to the strong VM isolation enforced by virtualization \cite{virt_app}.
\par
\begin{figure}[ht]
\centering
        \includegraphics[height=4cm,width=9cm]{images/live_migration.png}
    \caption{Live Migration - a VM is migrated while being suspended for a minimal amount of time.  The VM's memory is synchronized to the new server.  At a certain point, the source VM is suspended while register state and the last few pages of Memory are transferred.} 
    \label{fig:live_migration}
\end{figure}

With the increase in size of VM farms to thousands of hypervisors (physical servers), scalability and manageability becomes a major concern. In order to solve these problems, live migration of VMs was designed. Live migration allows VMs to be reorganized for improving reachability, fault-tolerance as well as load balancing within VM farms. The basic idea of live migration is depicted in Figure \ref{fig:live_migration}.  While there is the need to suspend the VM to copy over the register state, this downtime is on the order of tens of milliseconds (and likely getting better)\cite{live_migration}.

\par
\begin{figure*}[ht]
\centering
        \includegraphics[height=8cm,width=18cm]{images/gang_migration.jpg}
    \caption{Live Gang Migration - multiple nodes are migrated in concert. The main purpose of this work is to determine how much redundant memory pages can be exploited to increase the efficiency of this process.}
    \label{fig:gang_migration}
\end{figure*}
However, it is generally observed that a group of multiple VMs coordinate with each other in order to provide services to clients.  For example, in a Hadoop Map/Reduce kind of workload, the mappers and reducers are spread over a set of different servers and all these jobs coordinate with each other complete a particular computation on behalf of the user. Also, in web-server kinds of environments, the web-server, database server and other servers are spread over a set of VMs in order to service to client requests.  Thus, in order to ensure maximum performance and lower downtime for processing client requests, it can be beneficial to perform gang live migration of groups of machines in a cluster rather than a single live migration of individual VMs. Furthermore, many Service-Level-Agreements (SLAs) restrict maintenance periods and administrators may be required to move large portions of clusters in order to meet SLAs.  The concept of gang live migration is illustrated in Figure \ref{fig:gang_migration} \cite{live_gang}.


Though live migration can be a powerful tool in increasing the manageability of VM farms, in the trivial implementation (just migrate multiple machines the same way you migrate one) it comes at the cost of increased network traffic.  One solution to reduce this traffic is to exploit the redundancy of data across virtual machines by migrating groups of similar virtual machines.  Theoretically, there exists redundant memory content across VMs in the form of kernel code, kernel static data, application libraries as well as application binaries and data (e.g. web pages for stateless web servers).  However, in order to ensure that the whole process of exploiting redundancy for gang live migration is worthwhile, we need to first understand the nature of this redundancy.


This project aims at first developing an expression for VM memory similarity and then investigating different practical use cases to determine the amount of memory redundancy that exists across VMs in these cases. 

\section{Redundancy Evaluation Architecture}\label{sec:redundancy}

\subsection{Mathematical Definition for Redundancy}\label{sec:red_math}
In this section, we intend to give a succinct definition for what is meant by memory redundancy and similarity across Virtual Machines.  In all of our analysis, we define the similarity of N virtual machines' memory as a percentage value S:
\begin{equation}
S = \frac{|(P_A \mathbf{O} P_B ... \mathbf{O} P_N)|}{\sum\limits_{i=1}^{N}|P_i|}
\end{equation}\label{eqn:s}
where $P_i$ denotes the multiset of pages for VM $i$ (i.e. each element is the content of a specific page in memory) and we define the operator $\mathbf{O}$ as a sort of `intersection' of two multisets that will be defined and demonstrated momentarily.  



If we let the multisets of pages be represented as sets of ordered pairs where $\tup{P,n}$ denotes that page $P$ appears $n$ times in the multiset,\footnote{The authors would like to gratefully acknowledge the help of Asaf Karagila in formalizing this definition for the operator $\mathbf{O}$ and the suggestion for using the ordered pair description of a multiset.}
then we can say:
\begin{equation}\label{eqn:s}
A\mathrel{\mathbf{O}}B\equiv\{\tup{x,i+j}\mid\tup{x,i}\in A\land\tup{x,j}\in B\}.
\end{equation}
It is easy to see that since $+$ and $\land$ are associative, so is $\mathbf{O}$, and Equation \ref{eqn:s} provides the similarity fraction we wish to obtain, independent of the order in which we compute $\mathbf{O}$ across each multiset of pages.

The operator $\mathbf{O}$ is easily illustrated by a simple example that should clarify the above definition:

\begin{align*}
\textrm{Let } & A = \{1,1,2,5\},~B = \{1,2,3\}\\
\textrm{Then } &{\cal A} = \{\tup{1,2}, \tup{2,1}, \tup{5,1}\}\\
& {\cal B} = \{\tup{1,1}, \tup{2,1}, \tup{3,1}\}\\
{\cal A} \mathbf{O} {\cal B} &= \{\tup{1,3}, \tup{2,2}\} = \{1,1,1,2,2\}
\end{align*}

The example above may not be 100\% rigorous as we liberally switch between multiset and ordered pair representations, but the explanation still expresses the quantity we defined.  This metric is extremely useful when determining the amount of redundancy in a virtual machine as it quantifies the number of pages that are present across all VMs and how often they appear.  If we assume an ideal gang migrator can move each redundant page once across a slow network link, this number gives the amount of savings that can be expected.

It should be noted than since many zero pages exist and compression techniques could be used to avoid sending these pages altogether\cite{live_adaptive_compress}, we define another metric $S_z$:
\begin{equation}
S_z = \frac{|(P_A \mathbf{O} P_B ... \mathbf{O} P_N)|-N_z}{\sum\limits_{i=1}^{N}|P_i|-N_z}
\end{equation}
where $N_z$ is the total number of zero pages in all VMs:
\begin{equation}
N_z = \sum\limits_{i=1,Z\subseteq P_i}\left(|P_i\mathbf{O}Z|-1\right).
\end{equation}\label{eqn:sz}
In the equation above, $Z$ is the zero page and the $-1$ balances out the extra count we get from each $Z$ on the right-hand-side of $\mathbf{O}$.

\subsection{Implementation}\label{sec:red_implementation}
In order to compute $S$ and $S_z$ defined in the previous section, we needed to develop a framework.  A flow diagram for our measuring process is included in Figure \ref{fig:flow}.  We chose to use a Linux kernel module even though it does restrict the type of guest operating systems we can study.  However, the kernel module allowed us to test a greater variety of setups than if we had modified the hypervisor\footnote{Our current implementation, however, is restricted to the x86\_64 architecture since we use {\tt kmap()} to bring pages into virtual memory before computing the hash.  On other architectures (e.g. x86), the kernel has a concept of ``high'' memory and we cannot use the same method for calculating the hash of a physical page}(since we used testbeds involving KVM, Xen, and VirtualBox hypervisors).  Comparing the exact contents of every page in memory would require a massive amount of storage and immense computational power, so we instead used sha1 hash values for each page.  The probability of a collision is extremely low (each 1GB VM represents 262144 hash values with a 4KB page size) and the Linux kernel has a built-in crypto API,\footnote{\url{http://lxr.linux.no/linux+*/Documentation/crypto/api-intro.txt}} so we feel that this choice of hashing function has no negative impact on our results.  We standardized on 1GB VMs as much as possible, though exceptions will be noted throughout the write-up.

The process for calculating similarity is as follows (enumeration matches the flow chart in Figure \ref{fig:flow}):
\begin{enumerate}
  \item The user tells the kernel module (via a proc file)  to compute the hash of the VM's physical memory
  \item The kernel module then starts a kernel thread (kthread)
  \item The kthread iterates over the physical pages and computes the sha1 hash of every page
  \item When finished, the kernel module writes a special value to the proc file system
  \item The proc file can be polled from userspace so that the user can know when to collect the data
  \item The user starts the collection application
  \item The collection application communicates with the kernel module via a character device
  \item The saved hashes are mapped into userspace via the {\tt mmap} system call
  \item The application then reads the mapped data and outputs it to a file
  \item Another application takes the aggregated output files and selectively calculates $S$ and $S_z$ (defined in the previous Section) for the desired VM
  \item The result is obtained
\end{enumerate}
Clearly, most of these steps can be automated via simple shell scripting for semi-automated analysis.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{images/architecture.png}
  \caption{The architecture used to calculate similarity between virtual machines}\label{fig:flow}
\end{figure}

\section{Optimistic Upper Bounds}\label{sec:optimistic}
Before we proceed with applied use cases, we decided to get an upper bound for redundancy by testing a subset of ``ideal'' situations.  This step is very important for moving ahead with the overall redundancy study because it establishes the maximum possible benefits that can be gained out of exploiting redundancy for gang migration. If the maximum value falls below some acceptable threshold, it is not worthwhile investing in sophisticated mechanisms for improving group migration. This section describes two basic methods for establishing the optimistic upper bounds for the redundancy study.  We do not address what is an acceptable threshold as that usually is expressed in the form of an implementation specific cost function.

\subsection{Same VM image across multiple instances}
This section describes the first basic method used to establish an optimistic upper bound.  In this method, we used the same disk image to boot multiple cloud instances (using a custom Openstack\footnote{http://www.openstack.org} cloud). In order to perform redundancy evaluation, every cloud instance was booted up using a common disk image followed by a hashed snapshot of the physical pages of the VM when the system completed booting. A total of 40VM instances were booted from a common disk image and hashes were captured for each VM instance.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{images/samevm_multiplerestarts.pdf}
  \caption{Redundancy across multiple restarts of VMs from same image.  The calculations are cumulative, i.e. 40VMs means we calculated $S$ (blue line) and $S_z$ (green line) across all 40 hashed snapshots.}\label{fig:multiple_restarts}
\end{figure}

The similarity of multiple instances based on the same image is plotted in Figure \ref{fig:multiple_restarts}.  As can be seen from the figure, the amount of redundancy across different VMs when including the zero pages is nearly equal to $100$\%. However, the inclusion of zero pages biases the redundancy calculation since, as discussed earlier, special mechanisms like zero page compression can be used to optimize the live migration. The normalized value for non-zero duplicate memory gives the real value for optimistic upper bound on the amount of redundancy that can be achieved in the best possible case. It is clearly evident from the figure that $40$\% redundancy is the best case value for system using a common disk image to spawn multiple VMs (in this one sample set). Though there is a steep decrease in the amount of redundacy after the first few VM images, the value for the upper bound appears to start saturating (this calculation is quite time intensive and it can be repeated for an arbitrarily large set in the future to see when saturation would occur).


\subsection{Comparison of different variants of Linux on bootup}
This section describes the second set of experiments performed to obtain base case values for the amount of redundancy that can be exploited for group migration. In this method, we compared the redundancy that exists across different variants of Linux operating system at boot. We tested CentOS 6.4, Fedora 14, Scientific Linux 6.3 and Ubuntu 12.04.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{images/diff-var.pdf}
  \caption{Redundancy across different variants of Linux.  Values above the diagonal are calculated by $S_z$ and below the diagonal using $S$. The diagonal elements were calculated using $S_z$ with different instances.}\label{fig:diff_var}
\end{figure}

Figure \ref{fig:diff_var} presents a color map of a matrix representation of the similarity between different variants of Linux operating system. The diagonal represents the similarity between two different instances of the same operating system, calculated using $S_z$. The elements below the diagonal represent the redundancy across different variants including the zero pages i.e. the value for $S$ calculated by Equation \ref{eqn:s}. On the other hand, the elements above the diagonal represent the redundancy not including the zero pages i.e. the value for $S_z$ from Equation \ref{eqn:sz}.

As can be seen from the figure, two different instances of the same operating system show high similarity, nearly $30$\% to $54$\% and mirroring Figure \ref{fig:multiple_restarts}. The similarity between different variants including zero page count ranges from nearly $18$\% to $48$\%. However, if zero pages are not considered, then the similarity across different operating systems drops down to 10\% or lower.  It was shocking that for example CentOS and Scientific Linux, also differing only in minor version would have a rather lower similarity (especially considering they are both based on the same Upstream Vendor).  These two ranges bound the expected gains possible out of exploiting redundancy across different VMs.  Obviously, the real similarity would depend strongly upon the application code, libraries and other static data. 

\section{Stability Over Time}
This section presents a detailed report of the experiments performed to evaluate the stability (here defined as \textit{how much the system's memory image changes}) of various systems over a finite time period. We used four different systems for our study: an idle desktop system, a lightly loaded traffic server, a heavily loaded TCP client and a scientific computing workload.\footnote{http://folding.stanford.edu}


\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{images/vm_vs_time.pdf}
  \caption{VM image changes over time, each point is calculated against the first one for each application (e.g. 6 hours on the $x$ axis means the hashes at $t=6$ are compared with the hashes at $t=1$). Samples were taken every 15 minutes. Here, only similarity values not considering zero pages are shown.}\label{fig:vm_vs_time}
\end{figure}

Figure \ref{fig:vm_vs_time} presents the redundancy comparison of different types of VMs over time.  The workloads are:
\begin{enumerate}
  \item Folding at Home: distributed computing Molecular Dynamics simulation that is CPU bound
  \item Idle Desktop: Ubuntu 12.04 desktop left idle
  \item Light Traffic Server: Real server that serves DNS/puppet for roughly 10 nodes in an Openstack cloud
  \item TCP Client: Described in Section \ref{sec:client-server}
\end{enumerate}

As can be seen from Figure \ref{fig:vm_vs_time}, the convergence to some constant value after a certain period of time is completely workload dependent (which confirms common sense expectations) and largely absent at longer time scales.  Hence, trend is towards shows a gradually downward progression. Each workload also seems to have a different behavior.  For example, the Folding at Home workload is mostly stable, but has a sharp drop when the work unit finishes and the system has to write the output and download a new dataset (thereby effectively flushing the buffer cache and the work unit loaded in memory).  Even the idle desktop has a rather aggressive negative trend, and we propose calculating this metric could be useful in quantifying OS bloat (i.e. would we get the same trend with something like Slackware Linux?).  The TCP client seems to suffer the same problem as Folding when it comes to an instantaneous (within 15 minute) change of the system's memory.  This data clearly shows that \textit{when} you take a snapshot should be a critical part of migration scheduling.

\section{Streaming Client-Server}\label{sec:client-server}
This section details experiments performed using a client-server kind of architecture in order to identify the level of redundancy that can be exploited during live migration over servers with long-lived connections. We have chosen this particular client-server kind of use case because it is well suited in describing certain clustered applications.

\subsection{Experimental Setup}
The entire group of client-server machines was realized using a set of five VMs running Ubuntu 12.04. One VM hosted the server machine, whereas 4 other VMs hosted the client machines. In order to generate different streams of traffic, we used the open-source traffic generation tool Ostinato.\footnote{http://code.google.com/p/ostinato/}

Ostinato has two components: The first component is ostinato, which runs on the server machine and allows the server to control various ports at the server side as well as on the client side. The second component is called drone, which runs on the client machines and interacts with ostinato on the server to control different traffic streams on the client side. We used ostinato to generate random TCP traffic streams from the server to each of the clients.  Also, we initiated random UDP traffic streams from each of the client machines to the server machine. Overall, there were two traffic streams live at any given moment between each client-server pair (from the client's viewpoint: TCP downstream, UDP upstream).

\subsection{Results and Analysis}
We utilized our redundancy evaluation architecture to capture redundancy results along two dimensions.  One is by varying the number of clients from one to four and other is by capturing the system redundancy numbers at different time instances.

\begin{figure}[htbp]
\centering
        \includegraphics[width=0.4\textwidth]{images/client-server1.png}
    \caption{Duplicate memory with varying number of clients using Ostinato.}
    \label{fig:client-server1}
\end{figure}

Figure \ref{fig:client-server1} presents the results of our evaluation by varying the number of clients from one to four.  As can be seen from this figure, the number of duplicate pages including zero pages is around $30$\% - $55$\%.	 Using the metrics that do not count zero pages (e.g. $S_z$), it can be seen from the figure that the amount of redundancy across the system is at least $10$\%. As mentioned in the experimental setup, the data streams between the server and the clients were made up of completely random data. Hence, the numbers that we see for redundancy can be viewed as a lower bound on the amount of duplicate memory that can be found in a client-server streaming architecture. With servers and clients having more correlated data, we expect the amount of redundancy to increase.

\begin{figure}[htbp]
\centering
        \includegraphics[width=8cm]{images/client-server2.png}
    \caption{Duplicate memory within the system at different time instances for the client-server streaming test.}
    \label{fig:client-server2}
\end{figure}

Figure \ref{fig:client-server2} presents the results of redundancy evaluation captured for a system with five VMs(four clients and one server) at a time interval of 15 minutes. As can be seen from the figure, the values for redundancy across the system at different times remain almost the same for duplicate memory with and without zero pages under consideration. We propose two conclusions from this behavior. The first is that the system preferred using hot cache pages more than allocating new pages for assembling the random packets in memory. Hence, the total number of duplicate pages including zero pages remains almost the same. Second is that the amount of duplicate memory without zero pages remains almost the same and is roughly $10$\% and likely constitutes the static operating system state and application code. This verifies our initial assumption that this number forms a lower bound on the amount of redundancy that can be exploited with completely random data running across the VMs. As the application level commonality increases in the form of data and/or code, the amount of overall system redundancy that can be exploited should also increase.  Ostinato can replay PCAP files, so in future work we may be able to either generate new standardized use cases or replay some production traffic streams.

\section{Hadoop}
Hadoop is one of the major applications of cloud computing and as a result, there exist many virtual machines (e.g. Amazon's Elastic Map Reduce) running some portion of the Hadoop software stack.  Of particular interest, the Hadoop Distributed FileSystem (HDFS) maintains a number of redundant copies of data across multiple nodes.  While this redundancy results in a large similarity factor for disk images, one could expect that due to in-memory caches (e.g. the filesystem buffer cache and the Hadoop distributed cache) there should also be a significant amount of redundancy across memory images.  

\subsection{Experimental Setup}
It would be ideal to run an experiment on a  large multi-thousand node Hadoop cluster, but we did not have root access to such a cluster.  For our setup, we used The Vagrant\footnote{http://www.vagrantup.com} and the Cloudera Manager\footnote{http://www.cloudera.com/hadoop} to build and orchestrate a 5 node Map/Reduce cluster.  There were four ``datanodes'' (running Datanode and Tasktracker) and one ``controller'' (running the same thing as the datatnodes with NameNode, JobTracker, and Zookeeper).  For this experiment, each virtual machine was given 2GB of memory to give more realistic datapoints (the Cloudera Manager Map/Reduce setup doesn't even work with 1GB of memory in the default configuration).

For this test to be effective, we needed a benchmark and a sample dataset.  For the benchmark, we used the simplest Map/Reduce application, WordCount (which just counts how many times each word appears in the input).  The data consisted of 2224 ebooks downloaded from Project Gutenberg,\footnote{http://www.gutenberg.org} which totaled $\sim$ 800GB.

\subsection{Results and Analysis}\label{sec:hadoop_results}
The results from a simple trial involving WordCount over the Project Gutenberg books are tabulated in Table \ref{tab:hadoop}.  These are preliminary results and each entry in the table only represents one data point.  Not surprisingly, the cluster on a fresh boot is similar to the Optimistic Upper Bounds presented in Section \ref{sec:optimistic}.  Since the data is from just single runs, we can only estimate the actual values.  Using this viewpoint, we can see that on this cluster (we ran the job several times before taking data to warm up the cache) it seems that approximately 1/3 of the memory is redundant.  However, given that these VMs had 2GB of memory and the operating system (based on Figure \ref{fig:multiple_restarts}) occupies roughly 0.5$\cdot$1024MB=512MB), that means that the actual redundant user data in this specific case is only $\sim$ 164MB.

\begin{table*}
  \begin{tabular}{lccccccccccc}
    \toprule
    & \multicolumn{2}{c}{Fresh Boot} && \multicolumn{2}{c}{During Map} && \multicolumn{2}{c}{During Reduce} && \multicolumn{2}{c}{After finishing}\\
    \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
    & w/zeros & w/o zeros && w/zeros & w/o zeros && w/zeros & w/o zeros && w/zeros & w/o zeros\\
    \midrule
      datanodes & 84 & 42 && 34 & 25 && 36 & 27 && 33 & 23\\
      full cluster & 72 & 27 && 20 & 11 &&  22 & 11 && 20 & 7\\
    \bottomrule
  \end{tabular}
  \caption{Measurements of similarity across a 5 node Hadoop Map/Reduce \& HDFS cluster.  One node acts as the ``controller,'' w/zeros and w/o zeros denote the quantities $S$ and $S_z$ respectively.}\label{tab:hadoop}
\end{table*}

As discussed earlier, the Hadoop use case is very important and deserves its own full investigation.  For example, the following improvements could be considered when doing a full study of the Hadoop use case:
\begin{enumerate}
  \item The first priority is generating statistics rather than just printing one-off data points.
  \item HDFS is rack-aware, and the \textit{rack-id} of each server has implications regarding network bandwidth, but also where redundant data lies (By default, HDFS keeps two copies in the same rack and one copy in a different rack).  An implementation targeting Hadoop could consume this data when making migration decisions, but it would be beneficial to study the redundancy in this case and help inform whether this data would be helpful.
  \item More applications would be beneficial. WordCount does exercise the system, but it is likely not representative of all workloads.  To extend the WordCount use case, we were trying to load books from more languages (to hopefully generte more reduce jobs).
  \item Hadoop does a lot more than Map/Reduce nowadays and includes many high level tools built on top of Map/Reduce such as Pig Latin, Hive, Mahout, HBase, etc\ldots These should also be tested.
\end{enumerate}

\section{Conclusions and Future Work}
The main contribution of this work is the framework and tools we've presented for similarity/redundancy analysis.  This can be used in a variety of contexts (for example, moving VMs to machines using Kernel Shared Memory) Preliminary data was presented to help build some intuition on whether or not it's worth it to pursue efficient live migration algorithms based on redundancy.  It was shown that even for highly redundant use cases in 1GB Virtual Machines, one cannot expect to gain more than roughly 50\% similarity, even across identical disk images.

Some interesting questions that have been raised and need to be addressed:
\begin{itemize}
  \item An actual study of why the same image booted multiple times only contains roughly 40\% redundancy in the large number of VMs limit.  Identification of all this transient state may help to inform the numbers.   
  \item With larger user data sets and system memories, one can expect to gain more redundancy
  \item How can we run this study on production servers? Of particular interest would be webservers and an actual Hadoop cluster vs. the artificial test cases presented here (e.g. how representative is random streaming data for the client-server test or wordcount for Hadoop?).
  \item Unfortunately, while we believe the impact of our measurements to be small, we do snapshot inside the VM and therefore perturb the system.  It may be useful to investigate the tradeoffs between our method and doing calculations at the hypervisor level (for a cookie-cutter cloud provider, modifying the hypervisor would make sense)
\end{itemize}

Overall, while data is far from conclusive, it seems that the savings obtained may not be worth it.  However, it is important to consider that 30\% of 100GB (for moving a whole cluster) is still 30GB, which can result in some substantial traffic savings (especially if the user is billed on bandwidth). 


\bibliographystyle{abbrv}
\bibliography{cs598mcc_paper}

\end{document}
